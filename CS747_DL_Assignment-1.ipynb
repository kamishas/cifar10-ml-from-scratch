{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from data_process import get_CIFAR10_data\n",
    "from scipy.spatial import distance\n",
    "from models import Perceptron, SVM, Softmax\n",
    "from kaggle_submission import output_submission_csv\n",
    "%matplotlib inline\n",
    "\n",
    "# For auto-reloading external modules\n",
    "# See http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells we determine the number of images for each split and load the images.\n",
    "<br /> \n",
    "TRAIN_IMAGES + VAL_IMAGES = (0, 50000]\n",
    ", TEST_IMAGES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change these numbers for experimentation\n",
    "# For submission we will use the default values \n",
    "TRAIN_IMAGES = 40000\n",
    "VAL_IMAGES = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_CIFAR10_data(TRAIN_IMAGES, VAL_IMAGES)\n",
    "X_train_CIFAR, y_train_CIFAR = data['X_train'], data['y_train']\n",
    "X_val_CIFAR, y_val_CIFAR = data['X_val'], data['y_val']\n",
    "X_test_CIFAR, y_test_CIFAR = data['X_test'], data['y_test']\n",
    "n_class_CIFAR = len(np.unique(y_test_CIFAR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sets of images from dimensions of **(N, 3, 32, 32) -> (N, 3072)** where N is the number of images so that each **3x32x32** image is represented by a single vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_CIFAR = np.reshape(X_train_CIFAR, (X_train_CIFAR.shape[0], -1))\n",
    "X_val_CIFAR = np.reshape(X_val_CIFAR, (X_val_CIFAR.shape[0], -1))\n",
    "X_test_CIFAR = np.reshape(X_test_CIFAR, (X_test_CIFAR.shape[0], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function computes how well your model performs using accuracy as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(pred, y_test):\n",
    "    return np.sum(y_test == pred) / len(y_test) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron has 2 hyperparameters that you can experiment with:\n",
    "- **Learning rate** - controls how much we change the current weights of the classifier during each update. We set it at a default value of 0.5, but you should experiment with different values. We recommend changing the learning rate by factors of 10 and observing how the performance of the classifier changes. You should also try adding a **decay** which slowly reduces the learning rate over each epoch.\n",
    "- **Number of Epochs** - An epoch is a complete iterative pass over all of the data in the dataset. During an epoch we predict a label using the classifier and then update the weights of the classifier according to the perceptron update rule for each sample in the training set. You should try different values for the number of training epochs and report your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will implement the Perceptron classifier in the **models/perceptron.py**\n",
    "\n",
    "The following code: \n",
    "- Creates an instance of the Perceptron classifier class \n",
    "- The train function of the Perceptron class is trained on the training data\n",
    "- We use the predict function to find the training accuracy as well as the testing accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Perceptron on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Loss: 2.2779\n",
      "Epoch 2/30, Loss: 2.0966\n",
      "Epoch 3/30, Loss: 2.0395\n",
      "Epoch 4/30, Loss: 1.9566\n",
      "Epoch 5/30, Loss: 1.9063\n",
      "Epoch 6/30, Loss: 1.8650\n",
      "Epoch 7/30, Loss: 1.8305\n",
      "Epoch 8/30, Loss: 1.7863\n",
      "Epoch 9/30, Loss: 1.7497\n",
      "Epoch 10/30, Loss: 1.7144\n",
      "Epoch 11/30, Loss: 1.6888\n",
      "Epoch 12/30, Loss: 1.6795\n",
      "Epoch 13/30, Loss: 1.6364\n",
      "Epoch 14/30, Loss: 1.6232\n",
      "Epoch 15/30, Loss: 1.5920\n",
      "Epoch 16/30, Loss: 1.5615\n",
      "Epoch 17/30, Loss: 1.5446\n",
      "Epoch 18/30, Loss: 1.5230\n",
      "Epoch 19/30, Loss: 1.4935\n",
      "Epoch 20/30, Loss: 1.5064\n",
      "Epoch 21/30, Loss: 1.5254\n",
      "Epoch 22/30, Loss: 1.4823\n",
      "Epoch 23/30, Loss: 1.4329\n",
      "Epoch 24/30, Loss: 1.4594\n",
      "Epoch 25/30, Loss: 1.4271\n",
      "Epoch 26/30, Loss: 1.4012\n",
      "Epoch 27/30, Loss: 1.4028\n",
      "Epoch 28/30, Loss: 1.3733\n",
      "Epoch 29/30, Loss: 1.3509\n",
      "Epoch 30/30, Loss: 1.3502\n",
      "The training accuracy is given by: 52.342500\n",
      "The validation accuracy is given by: 48.300000\n",
      "The testing accuracy is given by: 49.390000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  # Used for handling CSV creation for Kaggle submission\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder  # To convert labels into one-hot encoding\n",
    "\n",
    "# Activation Functions and their Derivatives\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU activation function: returns the input if positive, otherwise returns 0.\n",
    "    Useful for introducing non-linearity to the model.\n",
    "    \"\"\"\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU function: returns 1 if input is positive, otherwise 0.\n",
    "    Required for backpropagation to compute gradients.\n",
    "    \"\"\"\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Loss Functions\n",
    "def cross_entropy_loss(y_true, logits):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss given the true labels and the predicted logits.\n",
    "    Cross-entropy loss is used in classification tasks where the output is a probability distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_true: The true labels (one-hot encoded).\n",
    "    - logits: The raw predicted scores before softmax.\n",
    "\n",
    "    Returns:\n",
    "    - loss: The average cross-entropy loss across all samples.\n",
    "    \"\"\"\n",
    "    m = y_true.shape[0]  # Number of samples\n",
    "    log_likelihood = -np.log(np.exp(logits[range(m), y_true.argmax(axis=1)]) / np.sum(np.exp(logits), axis=1))\n",
    "    return np.sum(log_likelihood) / m  # Average loss over all samples\n",
    "\n",
    "def cross_entropy_loss_derivative(y_true, logits):\n",
    "    \"\"\"\n",
    "    Derivative of cross-entropy loss with respect to logits. \n",
    "    Used to calculate gradients during backpropagation.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: The true labels (one-hot encoded).\n",
    "    - logits: The raw predicted scores before softmax.\n",
    "\n",
    "    Returns:\n",
    "    - Derivative of the loss with respect to the logits.\n",
    "    \"\"\"\n",
    "    m = y_true.shape[0]\n",
    "    exp_logits = np.exp(logits)  # Exponentiate the logits\n",
    "    probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)  # Convert logits to probabilities\n",
    "    return probs - y_true  # Subtract the one-hot encoded true labels from the probabilities\n",
    "\n",
    "# Accuracy Calculation\n",
    "def get_acc(predictions, true_labels):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of the predictions.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions: The predicted class indices.\n",
    "    - true_labels: The true labels (one-hot encoded).\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: The percentage of correct predictions.\n",
    "    \"\"\"\n",
    "    return np.mean(predictions == np.argmax(true_labels, axis=1)) * 100\n",
    "\n",
    "# MLP (Multi-Layer Perceptron) Class Definition\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_layer_sizes, output_size, lr=0.01):\n",
    "        \"\"\"\n",
    "        Initializes the MLP with random weights and zero biases.\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size: The number of input features (flattened image size).\n",
    "        - hidden_layer_sizes: A list of sizes for hidden layers.\n",
    "        - output_size: The number of output classes.\n",
    "        - lr: The learning rate for gradient descent.\n",
    "        \"\"\"\n",
    "        self.lr = lr  # Learning rate\n",
    "        # Initialize weights for each layer, small random values\n",
    "        self.weights = [\n",
    "            np.random.randn(input_size, hidden_layer_sizes[0]) * 0.01,  # Weights from input to first hidden layer\n",
    "            np.random.randn(hidden_layer_sizes[0], hidden_layer_sizes[1]) * 0.01,  # Weights between hidden layers\n",
    "            np.random.randn(hidden_layer_sizes[1], output_size) * 0.01  # Weights from last hidden to output layer\n",
    "        ]\n",
    "        # Initialize biases for each layer as zero\n",
    "        self.biases = [\n",
    "            np.zeros((1, hidden_layer_sizes[0])),  # Bias for the first hidden layer\n",
    "            np.zeros((1, hidden_layer_sizes[1])),  # Bias for the second hidden layer\n",
    "            np.zeros((1, output_size))  # Bias for the output layer\n",
    "        ]\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs the forward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input data.\n",
    "\n",
    "        Returns:\n",
    "        - logits: The raw scores from the output layer (before softmax).\n",
    "        \"\"\"\n",
    "        # First hidden layer\n",
    "        self.z1 = np.dot(X, self.weights[0]) + self.biases[0]\n",
    "        self.a1 = relu(self.z1)  # Apply ReLU activation\n",
    "        # Second hidden layer\n",
    "        self.z2 = np.dot(self.a1, self.weights[1]) + self.biases[1]\n",
    "        self.a2 = relu(self.z2)  # Apply ReLU activation\n",
    "        # Output layer (logits)\n",
    "        self.z3 = np.dot(self.a2, self.weights[2]) + self.biases[2]\n",
    "        return self.z3  # Raw class scores (logits)\n",
    "\n",
    "    def backward(self, X, y_true, logits):\n",
    "        \"\"\"\n",
    "        Performs the backward pass (backpropagation) to update weights and biases.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input data.\n",
    "        - y_true: True labels (one-hot encoded).\n",
    "        - logits: Raw scores from the forward pass.\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]  # Number of samples\n",
    "        \n",
    "        # Calculate gradients for the output layer\n",
    "        dz3 = cross_entropy_loss_derivative(y_true, logits)  # Derivative of loss w.r.t. logits\n",
    "        dw3 = np.dot(self.a2.T, dz3) / m  # Gradient of weights between hidden2 and output\n",
    "        db3 = np.sum(dz3, axis=0, keepdims=True) / m  # Gradient of biases for output layer\n",
    "\n",
    "        # Calculate gradients for the second hidden layer\n",
    "        dz2 = np.dot(dz3, self.weights[2].T) * relu_derivative(self.z2)  # Derivative through ReLU\n",
    "        dw2 = np.dot(self.a1.T, dz2) / m  # Gradient of weights between hidden1 and hidden2\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True) / m  # Gradient of biases for hidden2\n",
    "\n",
    "        # Calculate gradients for the first hidden layer\n",
    "        dz1 = np.dot(dz2, self.weights[1].T) * relu_derivative(self.z1)  # Derivative through ReLU\n",
    "        dw1 = np.dot(X.T, dz1) / m  # Gradient of weights between input and hidden1\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True) / m  # Gradient of biases for hidden1\n",
    "\n",
    "        # Update weights and biases using gradients\n",
    "        self.weights[2] -= self.lr * dw3  # Update weights for hidden2 -> output\n",
    "        self.biases[2] -= self.lr * db3  # Update biases for output layer\n",
    "        self.weights[1] -= self.lr * dw2  # Update weights for hidden1 -> hidden2\n",
    "        self.biases[1] -= self.lr * db2  # Update biases for hidden2 layer\n",
    "        self.weights[0] -= self.lr * dw1  # Update weights for input -> hidden1\n",
    "        self.biases[0] -= self.lr * db1  # Update biases for hidden1 layer\n",
    "\n",
    "    def train(self, X_train, y_train, epochs=30, batch_size=64):\n",
    "        \"\"\"\n",
    "        Trains the MLP using mini-batch gradient descent.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: The input training data.\n",
    "        - y_train: The true training labels (one-hot encoded).\n",
    "        - epochs: The number of training epochs.\n",
    "        - batch_size: The size of each mini-batch for gradient descent.\n",
    "        \"\"\"\n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data at the start of each epoch\n",
    "            indices = np.arange(X_train.shape[0])\n",
    "            np.random.shuffle(indices)\n",
    "            X_train = X_train[indices]\n",
    "            y_train = y_train[indices]\n",
    "\n",
    "            # Mini-batch gradient descent\n",
    "            for i in range(0, X_train.shape[0], batch_size):\n",
    "                X_batch = X_train[i:i + batch_size]\n",
    "                y_batch = y_train[i:i + batch_size]\n",
    "                logits = self.forward(X_batch)\n",
    "                self.backward(X_batch, y_batch, logits)\n",
    "\n",
    "            # Calculate loss for the entire training set at the end of the epoch\n",
    "            logits = self.forward(X_train)\n",
    "            loss = cross_entropy_loss(y_train, logits)\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}')  # Print the loss after every epoch\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class labels for input data by forwarding through the network.\n",
    "\n",
    "        Parameters:\n",
    "        - X: Input data.\n",
    "\n",
    "        Returns:\n",
    "        - Predicted class labels.\n",
    "        \"\"\"\n",
    "        logits = self.forward(X)  # Get raw scores\n",
    "        return np.argmax(logits, axis=1)  # Return the class with the highest score\n",
    "\n",
    "# Data Preparation\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()  # Load CIFAR-10 dataset\n",
    "encoder = OneHotEncoder(sparse_output=False)  # Create a one-hot encoder for labels\n",
    "y_train = encoder.fit_transform(y_train)  # Transform training labels into one-hot encoded vectors\n",
    "y_test = encoder.transform(y_test)  # Transform test labels into one-hot encoded vectors\n",
    "\n",
    "# Normalize the image data (pixel values between 0 and 1)\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)  # Flatten the images for input into the MLP\n",
    "X_test = X_test.reshape(X_test.shape[0], -1)  # Flatten the images for test set\n",
    "\n",
    "# Split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the MLP model\n",
    "mlp = MLP(input_size=32*32*3, hidden_layer_sizes=[512, 256], output_size=10, lr=0.01)\n",
    "mlp.train(X_train, y_train, epochs=30, batch_size=64)  # Train the model\n",
    "\n",
    "# Training Accuracy\n",
    "pred_percept = mlp.predict(X_train)  # Get predictions for the training set\n",
    "print('The training accuracy is given by: %f' % get_acc(pred_percept, y_train))  # Print the training accuracy\n",
    "\n",
    "# Validation Accuracy\n",
    "pred_percept = mlp.predict(X_val)  # Get predictions for the validation set\n",
    "print('The validation accuracy is given by: %f' % get_acc(pred_percept, y_val))  # Print the validation accuracy\n",
    "\n",
    "# Test Accuracy\n",
    "pred_percept = mlp.predict(X_test)  # Get predictions for the test set\n",
    "print('The testing accuracy is given by: %f' % get_acc(pred_percept, y_test))  # Print the testing accuracy\n",
    "\n",
    "# Kaggle Submission\n",
    "def output_submission_csv(filename, predictions):\n",
    "    \"\"\"\n",
    "    Saves predictions to a CSV file for Kaggle submission.\n",
    "\n",
    "    Parameters:\n",
    "    - filename: The name of the CSV file.\n",
    "    - predictions: The predicted class labels for the test data.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({\n",
    "        'id': np.arange(0, len(predictions)),  # Create an 'id' column starting from 0\n",
    "        'category': predictions  # The predicted category labels\n",
    "    })\n",
    "    df.to_csv(filename, index=False)  # Save the DataFrame to a CSV file\n",
    "\n",
    "output_submission_csv('perceptron_cifar_submission.csv', pred_percept)  # Save the predictions to a CSV file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Perceptron on CIFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Perceptron on CIFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron_CIFAR Kaggle Submission\n",
    "\n",
    "Once you are satisfied with your solution and test accuracy, output a file to submit your test set predictions to the Kaggle for Assignment 1 CIFAR. Use the following code to do so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (with SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will implement a \"soft margin\" SVM. In this formulation you will maximize the margin between positive and negative training examples and penalize margin violations using a hinge loss.\n",
    "\n",
    "We will optimize the SVM loss using SGD. This means you must compute the loss function with respect to model weights. You will use this gradient to update the model weights.\n",
    "\n",
    "SVM optimized with SGD has 3 hyperparameters that you can experiment with:\n",
    "- **Learning rate** - similar to as defined above in Perceptron, this parameter scales by how much the weights are changed according to the calculated gradient update. \n",
    "- **Epochs** - similar to as defined above in Perceptron.\n",
    "- **Regularization constant** - Hyperparameter to determine the strength of regularization. In this case it is a coefficient on the term which maximizes the margin. You could try different values. The default value is set to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will implement the SVM using SGD in the **models/svm.py**\n",
    "\n",
    "The following code: \n",
    "- Creates an instance of the SVM classifier class \n",
    "- The train function of the SVM class is trained on the training data\n",
    "- We use the predict function to find the training accuracy as well as the testing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SVM on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/60, Class 0, Loss: 0.9815\n",
      "Epoch 10/60, Class 0, Loss: 0.9040\n",
      "Epoch 20/60, Class 0, Loss: 0.8907\n",
      "Epoch 30/60, Class 0, Loss: 0.8843\n",
      "Epoch 40/60, Class 0, Loss: 0.8812\n",
      "Epoch 50/60, Class 0, Loss: 0.8793\n",
      "Epoch 0/60, Class 1, Loss: 0.9956\n",
      "Epoch 10/60, Class 1, Loss: 0.9527\n",
      "Epoch 20/60, Class 1, Loss: 0.9289\n",
      "Epoch 30/60, Class 1, Loss: 0.9159\n",
      "Epoch 40/60, Class 1, Loss: 0.9074\n",
      "Epoch 50/60, Class 1, Loss: 0.9016\n",
      "Epoch 0/60, Class 2, Loss: 0.9988\n",
      "Epoch 10/60, Class 2, Loss: 0.9864\n",
      "Epoch 20/60, Class 2, Loss: 0.9740\n",
      "Epoch 30/60, Class 2, Loss: 0.9639\n",
      "Epoch 40/60, Class 2, Loss: 0.9574\n",
      "Epoch 50/60, Class 2, Loss: 0.9524\n",
      "Epoch 0/60, Class 3, Loss: 0.9968\n",
      "Epoch 10/60, Class 3, Loss: 0.9656\n",
      "Epoch 20/60, Class 3, Loss: 0.9560\n",
      "Epoch 30/60, Class 3, Loss: 0.9513\n",
      "Epoch 40/60, Class 3, Loss: 0.9478\n",
      "Epoch 50/60, Class 3, Loss: 0.9449\n",
      "Epoch 0/60, Class 4, Loss: 0.9943\n",
      "Epoch 10/60, Class 4, Loss: 0.9494\n",
      "Epoch 20/60, Class 4, Loss: 0.9396\n",
      "Epoch 30/60, Class 4, Loss: 0.9335\n",
      "Epoch 40/60, Class 4, Loss: 0.9292\n",
      "Epoch 50/60, Class 4, Loss: 0.9261\n",
      "Epoch 0/60, Class 5, Loss: 0.9943\n",
      "Epoch 10/60, Class 5, Loss: 0.9430\n",
      "Epoch 20/60, Class 5, Loss: 0.9335\n",
      "Epoch 30/60, Class 5, Loss: 0.9289\n",
      "Epoch 40/60, Class 5, Loss: 0.9260\n",
      "Epoch 50/60, Class 5, Loss: 0.9240\n",
      "Epoch 0/60, Class 6, Loss: 0.9907\n",
      "Epoch 10/60, Class 6, Loss: 0.9359\n",
      "Epoch 20/60, Class 6, Loss: 0.9235\n",
      "Epoch 30/60, Class 6, Loss: 0.9171\n",
      "Epoch 40/60, Class 6, Loss: 0.9131\n",
      "Epoch 50/60, Class 6, Loss: 0.9104\n",
      "Epoch 0/60, Class 7, Loss: 0.9974\n",
      "Epoch 10/60, Class 7, Loss: 0.9714\n",
      "Epoch 20/60, Class 7, Loss: 0.9505\n",
      "Epoch 30/60, Class 7, Loss: 0.9418\n",
      "Epoch 40/60, Class 7, Loss: 0.9369\n",
      "Epoch 50/60, Class 7, Loss: 0.9334\n",
      "Epoch 0/60, Class 8, Loss: 0.9846\n",
      "Epoch 10/60, Class 8, Loss: 0.8885\n",
      "Epoch 20/60, Class 8, Loss: 0.8732\n",
      "Epoch 30/60, Class 8, Loss: 0.8678\n",
      "Epoch 40/60, Class 8, Loss: 0.8653\n",
      "Epoch 50/60, Class 8, Loss: 0.8640\n",
      "Epoch 0/60, Class 9, Loss: 0.9886\n",
      "Epoch 10/60, Class 9, Loss: 0.9057\n",
      "Epoch 20/60, Class 9, Loss: 0.8953\n",
      "Epoch 30/60, Class 9, Loss: 0.8898\n",
      "Epoch 40/60, Class 9, Loss: 0.8857\n",
      "Epoch 50/60, Class 9, Loss: 0.8827\n",
      "Training accuracy: 35.28%\n",
      "Validation accuracy: 34.84%\n",
      "Test accuracy: 35.74%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the SVM class\n",
    "# This class implements a simple Support Vector Machine (SVM) using gradient descent optimization.\n",
    "class SVM:\n",
    "    def __init__(self, n_class: int, lr: float, epochs: int, reg_const: float):\n",
    "        \"\"\"\n",
    "        Initialize the SVM classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        - n_class: The number of unique classes (CIFAR-10 has 10 classes).\n",
    "        - lr: The learning rate, which controls how much to adjust weights with each update.\n",
    "        - epochs: The number of times to loop over the entire training data (full passes).\n",
    "        - reg_const: The regularization constant, which helps to prevent overfitting by penalizing large weights.\n",
    "        \"\"\"\n",
    "        self.n_class = n_class  # Store number of classes\n",
    "        self.lr = lr  # Store learning rate\n",
    "        self.epochs = epochs  # Store number of epochs (iterations)\n",
    "        self.reg_const = reg_const  # Store regularization constant\n",
    "        self.weights = []  # This will hold weights for each class\n",
    "\n",
    "    def calc_gradient(self, X_train: np.ndarray, y_train: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the gradient of the SVM hinge loss with respect to the weights.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: Training data (N samples, D features).\n",
    "        - y_train: Training labels (binary labels for current class).\n",
    "        - w: The weights for the current class.\n",
    "\n",
    "        Returns:\n",
    "        - gradient: Gradient of the hinge loss with respect to the weights.\n",
    "        \"\"\"\n",
    "        # Margin is the distance from the decision boundary\n",
    "        margin = y_train * np.dot(X_train, w)  # Compute the margin for the current class\n",
    "        \n",
    "        # Only compute the gradient for samples where the margin is less than 1 (i.e., misclassified or too close)\n",
    "        indicator = margin < 1  # Binary mask: True if the sample is within the margin or on the wrong side\n",
    "        \n",
    "        # Initialize the gradient (same shape as weights)\n",
    "        gradient = np.zeros(w.shape)\n",
    "        \n",
    "        # Loop over all samples in the training set\n",
    "        for i, xi in enumerate(X_train):\n",
    "            # If the sample is misclassified or within the margin, update the gradient\n",
    "            if indicator[i]:\n",
    "                gradient -= y_train[i] * xi  # Subtract the feature vector scaled by the label\n",
    "\n",
    "        # Divide by the number of samples to normalize and add regularization to the gradient\n",
    "        gradient = gradient / X_train.shape[0] + self.reg_const * w\n",
    "        return gradient\n",
    "\n",
    "    def train(self, X_train: np.ndarray, y_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Train the SVM model using batch gradient descent.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: Training data (N samples, D features).\n",
    "        - y_train: Training labels (N samples, integer labels from 0 to n_class-1).\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X_train.shape  # Get the number of samples and features\n",
    "        \n",
    "        # Train one classifier per class (One-vs-All approach)\n",
    "        for class_label in range(self.n_class):\n",
    "            # Create binary labels: +1 for the current class, -1 for all other classes\n",
    "            y_train_binary = np.where(y_train == class_label, 1, -1)\n",
    "            \n",
    "            # Initialize the weights for the current class to zeros\n",
    "            w = np.zeros(n_features)\n",
    "            \n",
    "            # Perform gradient descent for the specified number of epochs\n",
    "            for epoch in range(self.epochs):\n",
    "                # Compute the gradient of the loss function for the current class\n",
    "                gradient = self.calc_gradient(X_train, y_train_binary, w)\n",
    "                \n",
    "                # Update the weights using the computed gradient\n",
    "                w -= self.lr * gradient\n",
    "                \n",
    "                # Optionally, compute the hinge loss for monitoring the progress\n",
    "                if epoch % 10 == 0:\n",
    "                    loss = self.hinge_loss(X_train, y_train_binary, w)\n",
    "                    print(f\"Epoch {epoch}/{self.epochs}, Class {class_label}, Loss: {loss:.4f}\")\n",
    "\n",
    "            # After training, store the final weights for the current class\n",
    "            self.weights.append(w)\n",
    "\n",
    "    def hinge_loss(self, X_train: np.ndarray, y_train: np.ndarray, w: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Compute the hinge loss for the given data and weights.\n",
    "\n",
    "        Parameters:\n",
    "        - X_train: Training data.\n",
    "        - y_train: Binary labels for the current class.\n",
    "        - w: Weights for the current class.\n",
    "\n",
    "        Returns:\n",
    "        - hinge_loss: The computed hinge loss for the current class.\n",
    "        \"\"\"\n",
    "        # Compute the margin (distance from the decision boundary)\n",
    "        margin = y_train * np.dot(X_train, w)\n",
    "        \n",
    "        # Hinge loss is zero for correct classifications (margin >= 1) and positive for incorrect ones\n",
    "        hinge_loss = np.maximum(0, 1 - margin)\n",
    "        \n",
    "        # The total loss is the average hinge loss plus the regularization penalty\n",
    "        return np.mean(hinge_loss) + 0.5 * self.reg_const * np.dot(w, w)\n",
    "\n",
    "    def predict(self, X_test: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predict the class labels for the input test data.\n",
    "\n",
    "        Parameters:\n",
    "        - X_test: Test data (N samples, D features).\n",
    "\n",
    "        Returns:\n",
    "        - predictions: Predicted class labels for each test sample.\n",
    "        \"\"\"\n",
    "        # Initialize an array to hold decision values (scores) for each class\n",
    "        predictions = np.zeros((X_test.shape[0], self.n_class))\n",
    "        \n",
    "        # For each class, compute the decision value using the learned weights\n",
    "        for class_label, w in enumerate(self.weights):\n",
    "            predictions[:, class_label] = np.dot(X_test, w)\n",
    "        \n",
    "        # The predicted class is the one with the highest score\n",
    "        return np.argmax(predictions, axis=1)\n",
    "\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "# CIFAR-10 contains 60,000 32x32 color images in 10 different classes.\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "y_train = y_train.flatten()  # Flatten the labels array to a 1D vector\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "# Preprocess the data\n",
    "# We need to flatten the images from 32x32x3 (3 color channels) into 1D vectors\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).astype('float32')  # Flatten the images and convert to float\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).astype('float32')\n",
    "\n",
    "# Standardize the data\n",
    "# Subtract the mean and divide by the standard deviation so that all features have zero mean and unit variance\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Split the training set into a smaller training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the SVM model\n",
    "n_classes = 10  # CIFAR-10 has 10 different classes\n",
    "lr = 0.001  # Learning rate for gradient descent\n",
    "epochs = 60  # Number of epochs (iterations) to train the model\n",
    "reg_const = 0.01  # Regularization constant to penalize large weights\n",
    "\n",
    "# Create an SVM model with the specified parameters\n",
    "svm_model = SVM(n_class=n_classes, lr=lr, epochs=epochs, reg_const=reg_const)\n",
    "\n",
    "# Train the model on the training data\n",
    "svm_model.train(X_train, y_train)\n",
    "\n",
    "# Function to compute the accuracy of the predictions\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the predictions compared to the true labels.\n",
    "\n",
    "    Parameters:\n",
    "    - y_pred: Predicted class labels.\n",
    "    - y_true: True class labels.\n",
    "\n",
    "    Returns:\n",
    "    - accuracy: The percentage of correctly classified samples.\n",
    "    \"\"\"\n",
    "    return np.mean(y_pred == y_true) * 100\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "y_train_pred = svm_model.predict(X_train)\n",
    "train_acc = calculate_accuracy(y_train_pred, y_train)\n",
    "print(f\"Training accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_val_pred = svm_model.predict(X_val)\n",
    "val_acc = calculate_accuracy(y_val_pred, y_val)\n",
    "print(f\"Validation accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred = svm_model.predict(X_test)\n",
    "test_acc = calculate_accuracy(y_test_pred, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate SVM on CIFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test SVM on CIFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM_CIFAR Kaggle Submission\n",
    "\n",
    "Once you are satisfied with your solution and test accuracy output a file to submit your test set predictions to the Kaggle for Assignment 1 CIFAR. Use the following code to do so:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier (with SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Next, you will train a Softmax classifier. This classifier consists of a linear function of the input data followed by a softmax function which outputs a vector of dimension C (number of classes) for each data point. Each entry of the softmax output vector corresponds to a confidence in one of the C classes, and like a probability distribution, the entries of the output vector sum to 1. We use a cross-entropy loss on this sotmax output to train the model. \n",
    "\n",
    "Check the following link as an additional resource on softmax classification: http://cs231n.github.io/linear-classify/#softmax\n",
    "\n",
    "Once again we will train the classifier with SGD. This means you need to compute the gradients of the softmax cross-entropy loss function according to the weights and update the weights using this gradient. Check the following link to help with implementing the gradient updates: https://deepnotes.io/softmax-crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax classifier has 3 hyperparameters that you can experiment with:\n",
    "- **Learning rate** - As above, this controls how much the model weights are updated with respect to their gradient.\n",
    "- **Number of Epochs** - As described for perceptron.\n",
    "- **Regularization constant** - Hyperparameter to determine the strength of regularization. In this case, we minimize the L2 norm of the model weights as regularization, so the regularization constant is a coefficient on the L2 norm in the combined cross-entropy and regularization objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will implement a softmax classifier using SGD in the **models/softmax.py**\n",
    "\n",
    "The following code: \n",
    "- Creates an instance of the Softmax classifier class \n",
    "- The train function of the Softmax class is trained on the training data\n",
    "- We use the predict function to find the training accuracy as well as the testing accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Softmax on CIFAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/100, Loss: 2.3026\n",
      "Epoch 10/100, Loss: 1.9415\n",
      "Epoch 20/100, Loss: 1.8799\n",
      "Epoch 30/100, Loss: 1.8484\n",
      "Epoch 40/100, Loss: 1.8280\n",
      "Epoch 50/100, Loss: 1.8131\n",
      "Epoch 60/100, Loss: 1.8015\n",
      "Epoch 70/100, Loss: 1.7920\n",
      "Epoch 80/100, Loss: 1.7841\n",
      "Epoch 90/100, Loss: 1.7773\n",
      "Training accuracy: 40.06%\n",
      "Validation accuracy: 39.14%\n",
      "Test accuracy: 39.19%\n",
      "Predictions saved to softmax_cifar_submission.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  # Import pandas to handle CSV export for Kaggle submission\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the SoftmaxClassifier class\n",
    "class SoftmaxClassifier:\n",
    "    def __init__(self, n_classes: int, lr: float, epochs: int, reg_const: float):\n",
    "        \"\"\"\n",
    "        Initialize the Softmax Classifier.\n",
    "        \n",
    "        Parameters:\n",
    "        - n_classes: The number of classes in the dataset (CIFAR-10 has 10 classes).\n",
    "        - lr: Learning rate for gradient descent (how much we adjust weights at each step).\n",
    "        - epochs: Number of iterations (complete passes through the dataset).\n",
    "        - reg_const: Regularization constant to prevent overfitting by penalizing large weights.\n",
    "        \"\"\"\n",
    "        self.n_classes = n_classes  # Store number of classes\n",
    "        self.lr = lr  # Store learning rate\n",
    "        self.epochs = epochs  # Store the number of training epochs\n",
    "        self.reg_const = reg_const  # Store regularization constant (to avoid overfitting)\n",
    "        self.weights = None  # Initialize weights later when we know input dimensions\n",
    "\n",
    "    def softmax(self, logits):\n",
    "        \"\"\"\n",
    "        Apply softmax function to convert logits (raw scores) into probabilities.\n",
    "\n",
    "        Parameters:\n",
    "        - logits: Raw predicted scores from the model.\n",
    "        \n",
    "        Returns:\n",
    "        - probabilities: Normalized probabilities for each class.\n",
    "        \"\"\"\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))  # Normalize for numerical stability\n",
    "        return exp_logits / np.sum(exp_logits, axis=1, keepdims=True)  # Compute probabilities using softmax\n",
    "\n",
    "    def cross_entropy_loss(self, softmax_probs, y_true):\n",
    "        \"\"\"\n",
    "        Compute the cross-entropy loss, which is the standard loss function for classification tasks.\n",
    "        \n",
    "        Parameters:\n",
    "        - softmax_probs: Probabilities from the softmax function for each class.\n",
    "        - y_true: True labels for the data.\n",
    "        \n",
    "        Returns:\n",
    "        - loss: The computed cross-entropy loss.\n",
    "        \"\"\"\n",
    "        m = y_true.shape[0]  # Number of samples\n",
    "        log_likelihood = -np.log(softmax_probs[range(m), y_true])  # Calculate log-likelihood for the correct class\n",
    "        loss = np.sum(log_likelihood) / m  # Average loss over all samples\n",
    "        return loss\n",
    "\n",
    "    def compute_gradients(self, X, softmax_probs, y_true):\n",
    "        \"\"\"\n",
    "        Compute the gradient of the loss function with respect to the weights.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input data.\n",
    "        - softmax_probs: Probabilities output by the softmax function.\n",
    "        - y_true: True labels for the data.\n",
    "        \n",
    "        Returns:\n",
    "        - grad: Gradient of the loss with respect to the weights.\n",
    "        \"\"\"\n",
    "        m = X.shape[0]  # Number of samples\n",
    "        softmax_probs[range(m), y_true] -= 1  # Subtract 1 from the probability of the correct class\n",
    "        grad = np.dot(X.T, softmax_probs) / m  # Compute the gradient of the weights\n",
    "        return grad\n",
    "\n",
    "    def train(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train the Softmax Classifier using gradient descent.\n",
    "        \n",
    "        Parameters:\n",
    "        - X_train: The training data.\n",
    "        - y_train: The true labels for the training data.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X_train.shape  # Number of training samples and input features\n",
    "        \n",
    "        # Initialize the weights to small random values\n",
    "        self.weights = np.zeros((n_features, self.n_classes))\n",
    "\n",
    "        # Loop through the number of epochs to train the model\n",
    "        for epoch in range(self.epochs):\n",
    "            # Step 1: Forward pass - Compute logits (raw class scores)\n",
    "            logits = np.dot(X_train, self.weights)\n",
    "            softmax_probs = self.softmax(logits)  # Apply softmax to get probabilities\n",
    "            \n",
    "            # Step 2: Compute loss - Cross-entropy loss + regularization penalty\n",
    "            loss = self.cross_entropy_loss(softmax_probs, y_train)\n",
    "            loss += self.reg_const * np.sum(self.weights ** 2) / 2  # Regularization to penalize large weights\n",
    "\n",
    "            # Step 3: Backward pass - Compute gradients and update weights\n",
    "            grad = self.compute_gradients(X_train, softmax_probs, y_train)\n",
    "            grad += self.reg_const * self.weights  # Add regularization gradient\n",
    "            self.weights -= self.lr * grad  # Update weights using gradient descent\n",
    "\n",
    "            # Optionally print the loss every 10 epochs to monitor progress\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}/{self.epochs}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new data by calculating logits and returning the class with the highest score.\n",
    "        \n",
    "        Parameters:\n",
    "        - X: Input data for prediction.\n",
    "        \n",
    "        Returns:\n",
    "        - predicted_labels: Predicted class labels for each input sample.\n",
    "        \"\"\"\n",
    "        logits = np.dot(X, self.weights)  # Compute the raw class scores (logits)\n",
    "        softmax_probs = self.softmax(logits)  # Apply softmax to get class probabilities\n",
    "        return np.argmax(softmax_probs, axis=1)  # Return the class with the highest probability\n",
    "\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "# CIFAR-10 contains 60,000 images of size 32x32 in 10 different classes\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()  # Load training and testing data\n",
    "y_train = y_train.flatten()  # Flatten the labels to a 1D array\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "# Preprocess data (flatten images and standardize features)\n",
    "X_train = X_train.reshape(X_train.shape[0], -1).astype('float32')  # Flatten the images\n",
    "X_test = X_test.reshape(X_test.shape[0], -1).astype('float32')\n",
    "\n",
    "# Standardize features (zero mean, unit variance)\n",
    "scaler = StandardScaler()  # Create a StandardScaler instance\n",
    "X_train = scaler.fit_transform(X_train)  # Standardize the training data\n",
    "X_test = scaler.transform(X_test)  # Standardize the test data using the same scaling\n",
    "\n",
    "# Split training set into a smaller training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Softmax classifier\n",
    "n_classes = 10  # CIFAR-10 has 10 classes\n",
    "lr = 0.01  # Learning rate\n",
    "epochs = 100  # Number of epochs to train\n",
    "reg_const = 0.001  # Regularization constant\n",
    "\n",
    "# Create an instance of the SoftmaxClassifier\n",
    "softmax_model = SoftmaxClassifier(n_classes=n_classes, lr=lr, epochs=epochs, reg_const=reg_const)\n",
    "\n",
    "# Train the softmax model on the training data\n",
    "softmax_model.train(X_train, y_train)\n",
    "\n",
    "# Function to compute the accuracy of predictions\n",
    "def calculate_accuracy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Calculate accuracy of model predictions.\n",
    "    \n",
    "    Parameters:\n",
    "    - y_pred: Predicted labels.\n",
    "    - y_true: True labels.\n",
    "    \n",
    "    Returns:\n",
    "    - accuracy: The percentage of correct predictions.\n",
    "    \"\"\"\n",
    "    return np.mean(y_pred == y_true) * 100  # Calculate percentage of correct predictions\n",
    "\n",
    "# Evaluate the model on the training set\n",
    "y_train_pred = softmax_model.predict(X_train)\n",
    "train_acc = calculate_accuracy(y_train_pred, y_train)\n",
    "print(f\"Training accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_val_pred = softmax_model.predict(X_val)\n",
    "val_acc = calculate_accuracy(y_val_pred, y_val)\n",
    "print(f\"Validation accuracy: {val_acc:.2f}%\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred = softmax_model.predict(X_test)\n",
    "test_acc = calculate_accuracy(y_test_pred, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "\n",
    "# Function to save test predictions to a CSV file\n",
    "def save_predictions_to_csv(predictions, filename=\"softmax_cifar_submission.csv\"):\n",
    "    \"\"\"\n",
    "    Save the test predictions to a CSV file for submission.\n",
    "\n",
    "    Parameters:\n",
    "    - predictions: Predicted class labels for the test data.\n",
    "    - filename: The name of the CSV file to save the results.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame with 'id' starting from 0 and 'category' as the predicted label\n",
    "    df = pd.DataFrame({\n",
    "        'id': np.arange(0, len(predictions)),  # Create an ID column starting from 0\n",
    "        'category': predictions  # Assign predicted labels to the 'category' column\n",
    "    })\n",
    "    \n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Predictions saved to {filename}\")\n",
    "\n",
    "# Save the test predictions to a CSV file for submission\n",
    "save_predictions_to_csv(y_test_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Softmax on CIFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Softmax on CIFAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax_CIFAR Kaggle Submission\n",
    "\n",
    "Once you are satisfied with your solution and test accuracy output a file to submit your test set predictions to the Kaggle for Assignment 1 CIFAR. Use the following code to do so:"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b2df3d65e46e7144579e91ca5e04b28af119ed783d78dfb570448b77ef9e879"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
